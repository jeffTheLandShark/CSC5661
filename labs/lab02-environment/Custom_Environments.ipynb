{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "991938ed",
   "metadata": {},
   "source": [
    "## Environment 1: Racetrack\n",
    "\n",
    "Consider driving a race car around a turn. You want\n",
    "to go as fast as possible, but not so fast as to run off the track. \n",
    "\n",
    "The racetrack is a grid\n",
    "– it begins with a vertical section 10 cells wide and 30 cells high, followed by a right turn\n",
    "into a horizontal section 15 cells wide and 10 cells tall. \n",
    "\n",
    "The starting line is the row of cells\n",
    "at the bottom of the first section. \n",
    "\n",
    "The finish line is the column of cells at the right of the\n",
    "horizontal section. \n",
    "\n",
    "The car begins at any cell on the starting line and each turn it is at one\n",
    "of the grid positions. \n",
    "\n",
    "The velocity is also discrete, a number of grid cells moved horizontally\n",
    "and vertically per time step. \n",
    "\n",
    "The actions are increments to the velocity components. \n",
    "\n",
    "Each\n",
    "may be changed by +1, −1, or 0 in each step, for a total of nine (3 ×3) actions. \n",
    "\n",
    "The\n",
    "vertical velocity component is restricted to be nonnegative. \n",
    "\n",
    "Both velocity components are\n",
    "restricted to be less than 5, and they cannot both be zero except at the starting line. \n",
    "\n",
    "Each\n",
    "episode begins in one of the randomly selected start states with both velocity components\n",
    "zero and ends when the car crosses the finish line. \n",
    "\n",
    "If the car attempts to go through the\n",
    "track boundary (anywhere by the finish line) it crashes and the episode ends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c02ff32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "class RacetrackEnv:\n",
    "    def __init__(self, config):\n",
    "        self.track_width = config[\"width\"]\n",
    "\n",
    "        self.x_max = self.track_width + config[\"turn\"]\n",
    "        self.y_max = self.track_width + config[\"straight\"]\n",
    "\n",
    "        self.x_inner = range(self.track_width, self.x_max)\n",
    "        self.y_inner = range(0, config[\"straight\"])\n",
    "\n",
    "        self.action_space = [(dx, dy) for dx in [-1, 0, 1] for dy in [-1, 0, 1]]\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self, seed=None):\n",
    "        self.velocity = (0, 0)\n",
    "        self.position = (random.randint(0, self.track_width - 1), 0)\n",
    "\n",
    "    def __check_bounds(self, position) -> bool:\n",
    "        return (\n",
    "            0 <= position[0] < self.x_max\n",
    "            and 0 <= position[1] < self.y_max\n",
    "            and not (position[0] in self.x_inner and position[1] in self.y_inner)\n",
    "        )\n",
    "\n",
    "    def __bound(self, val, min_val, max_val):\n",
    "        return max(min_val, min(val, max_val))\n",
    "\n",
    "    def step(self, action):\n",
    "        if 0 > action or action >= len(self.action_space):\n",
    "            raise ValueError(\"Invalid action\")\n",
    "        # get acceleration from action\n",
    "        acceleration = self.action_space[action]\n",
    "        # apply acceleration with bounds\n",
    "        self.velocity = (\n",
    "            self.__bound(self.velocity[0] + acceleration[0], 1, 5),\n",
    "            self.__bound(self.velocity[1] + acceleration[1], -5, 5),\n",
    "        )\n",
    "\n",
    "        # update position\n",
    "        new_position = (\n",
    "            self.position[0] + self.velocity[1],\n",
    "            self.position[1] + self.velocity[0],\n",
    "        )\n",
    "\n",
    "        if not self.__check_bounds(new_position):\n",
    "            # reset if out of bounds\n",
    "            # self.reset()\n",
    "            return {\"state\": [self.position, self.velocity], \"r\": -1, \"done\": True}\n",
    "\n",
    "        self.position = new_position\n",
    "        done = self.position[1] >= self.y_max - 1\n",
    "        reward = 5 if done else 1\n",
    "        return {\"state\": [self.position, self.velocity], \"r\": reward, \"done\": done}\n",
    "\n",
    "    def render(self, past_positions=None):\n",
    "        # generated using copilot auto-complete\n",
    "        track = [[\".\"] * self.x_max for _ in range(self.y_max)]\n",
    "        for x in self.x_inner:\n",
    "            for y in self.y_inner:\n",
    "                track[y][x] = \"#\"\n",
    "        if past_positions:\n",
    "            for pos in past_positions:\n",
    "                track[pos[1]][pos[0]] = \"*\"\n",
    "        track[self.position[1]][self.position[0]] = \"X\"\n",
    "        print(\"\\n\".join(\"\".join(row) for row in reversed(track)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa071af3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward over 1000 episodes: 4.78\n",
      "Max reward over 1000 episodes: 27\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    \"width\": 10,\n",
    "    \"straight\": 30,\n",
    "    \"turn\": 15,\n",
    "}\n",
    "num_episodes = 1000\n",
    "\n",
    "env = RacetrackEnv(config)\n",
    "results = []\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    past_positions = []\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        action = random.randint(0, len(env.action_space) - 1)\n",
    "        result = env.step(action)\n",
    "        state, reward, done = result[\"state\"], result[\"r\"], result[\"done\"]\n",
    "        past_positions.append(env.position)\n",
    "        total_reward += reward\n",
    "    results.append(total_reward)\n",
    "    # env.render(past_positions)\n",
    "\n",
    "print(f\"Average reward over {num_episodes} episodes: {sum(results) / len(results)}\")\n",
    "print(f\"Max reward over {num_episodes} episodes: {max(results)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7210f2b",
   "metadata": {},
   "source": [
    "## Environment 2 – a chase. \n",
    "\n",
    "Consider a chase on a square field with each side of length\n",
    "n. \n",
    "\n",
    "In the field is a predator and a player. \n",
    "\n",
    "The player begins in cell position (0, 0). \n",
    "\n",
    "The\n",
    "predator begins in a randomly generated cell position. At position (n, n) is the player’s base.\n",
    "\n",
    "\n",
    "Unlike the racetrack example, the player and predator can be at any location in the field,\n",
    "i.e. any real number between 0 and n. \n",
    "\n",
    "The velocity of the player is continuous, a distance\n",
    "moved horizontally and vertically per time step. \n",
    "\n",
    "The actions are increments to the velocity\n",
    "components.\n",
    "\n",
    "Each may be changed by +1, −1, or 0 in each step, for a total of nine (3 ×3)\n",
    "actions.\n",
    "\n",
    " In this case they are stochastic; given a horizontal velocity at time t of V (t)\n",
    "\n",
    "-----\n",
    "\n",
    "The vertical velocity is the same. \n",
    "\n",
    "Both velocity components are restricted to be no more\n",
    "than 5. \n",
    "\n",
    "Each turn, randomly choose either the predator or the player to move first. \n",
    "\n",
    "The\n",
    "predator moves a distance of no more than 4 directly toward the player.\n",
    "\n",
    " The player moves\n",
    "according to their current velocity (but must stay within the field of play). \n",
    "\n",
    "If the predator\n",
    "lands on the cell occupied by the player then the player is caught and the episode ends. \n",
    "\n",
    "If\n",
    "the player is within a distance of 0.5 of their base without being caught then they escape\n",
    "and the episode ends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb579fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "\n",
    "\n",
    "class ChaseEnv:\n",
    "    def __init__(self, config):\n",
    "        self.n = config[\"length\"]\n",
    "        self.pred_step_size = config[\"predator_step_size\"]\n",
    "        self.base_pos = (self.n - 1, self.n - 1)\n",
    "\n",
    "        self.action_space = [(dx, dy) for dx in [-1, 0, 1] for dy in [-1, 0, 1]]\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self, seed=None):\n",
    "        # reset positions\n",
    "        self.player_pos = (0, 0)\n",
    "        self.pred_pos = (random.randint(0, self.n - 1), random.randint(0, self.n - 1))\n",
    "        # reset velocities\n",
    "        self.player_vel = (0, 0)\n",
    "\n",
    "    def __bound(self, val, min_val, max_val):\n",
    "        return max(min_val, min(val, max_val))\n",
    "\n",
    "    def __move_player(self, acceleration):\n",
    "        # a = 0.5 to 1.5 * action acceleration\n",
    "        stochastic_fun = lambda x: x * (random.random() + 0.5)\n",
    "        stochastic_accel = (\n",
    "            stochastic_fun(acceleration[0]),\n",
    "            stochastic_fun(acceleration[1]),\n",
    "        )\n",
    "\n",
    "        # apply acceleration with bounds\n",
    "        self.player_vel = (\n",
    "            self.__bound(self.player_vel[0] + stochastic_accel[0], -5, 5),\n",
    "            self.__bound(self.player_vel[1] + stochastic_accel[1], -5, 5),\n",
    "        )\n",
    "        # update position with bounds\n",
    "        self.player_pos = (\n",
    "            self.__bound(self.player_pos[0] + self.player_vel[0], 0, self.n - 1),\n",
    "            self.__bound(self.player_pos[1] + self.player_vel[1], 0, self.n - 1),\n",
    "        )\n",
    "\n",
    "    def __move_predator(self):\n",
    "        # move towards player\n",
    "        dx = self.player_pos[0] - self.pred_pos[0]\n",
    "        dy = self.player_pos[1] - self.pred_pos[1]\n",
    "        # normalize and scale to step size\n",
    "        dist = math.hypot(dx, dy)\n",
    "        dx = (dx / dist) * self.pred_step_size if dist != 0 else 0\n",
    "        dy = (dy / dist) * self.pred_step_size if dist != 0 else 0\n",
    "        # update predeator position\n",
    "        self.pred_pos = (\n",
    "            self.__bound(self.pred_pos[0] + dx, 0, self.n - 1),\n",
    "            self.__bound(self.pred_pos[1] + dy, 0, self.n - 1),\n",
    "        )\n",
    "\n",
    "    def step(self, action):\n",
    "        # get new player velocity\n",
    "        if 0 > action or action >= len(self.action_space):\n",
    "            raise ValueError(\"Invalid action\")\n",
    "\n",
    "        # update positions w/ random who goes first\n",
    "        if random.random() < 0.5:\n",
    "            self.__move_player(self.action_space[action])\n",
    "            if math.dist(self.player_pos, self.base_pos) < 0.5:\n",
    "                return {\n",
    "                    \"state\": [self.player_pos, self.player_vel, self.pred_pos],\n",
    "                    \"r\": 10,\n",
    "                    \"done\": True,\n",
    "                }\n",
    "            self.__move_predator()\n",
    "            if math.dist(self.pred_pos, self.player_pos) < 0.01:\n",
    "                return {\n",
    "                    \"state\": [self.player_pos, self.player_vel, self.pred_pos],\n",
    "                    \"r\": -10,\n",
    "                    \"done\": True,\n",
    "                }\n",
    "        else:\n",
    "            self.__move_predator()\n",
    "            if math.dist(self.pred_pos, self.player_pos) < 0.01:\n",
    "                return {\n",
    "                    \"state\": [self.player_pos, self.player_vel, self.pred_pos],\n",
    "                    \"r\": -10,\n",
    "                    \"done\": True,\n",
    "                }\n",
    "            self.__move_player(self.action_space[action])\n",
    "            if math.dist(self.player_pos, self.base_pos) < 0.5:\n",
    "                return {\n",
    "                    \"state\": [self.player_pos, self.player_vel, self.pred_pos],\n",
    "                    \"r\": 10,\n",
    "                    \"done\": True,\n",
    "                }\n",
    "        return {\n",
    "            \"state\": [self.player_pos, self.player_vel, self.pred_pos],\n",
    "            \"r\": -1,\n",
    "            \"done\": False,\n",
    "        }\n",
    "\n",
    "    def render(self, past_positions=None):\n",
    "        # generated using copilot auto-complete\n",
    "        grid = [[\".\"] * self.n for _ in range(self.n)]\n",
    "        if past_positions:\n",
    "            for pos in past_positions:\n",
    "                grid[int(pos[0][1])][int(pos[0][0])] = \"*\"\n",
    "                grid[int(pos[1][1])][int(pos[1][0])] = \"o\"\n",
    "        grid[self.base_pos[1]][self.base_pos[0]] = \"B\"\n",
    "        grid[int(self.pred_pos[1])][int(self.pred_pos[0])] = \"P\"\n",
    "        grid[int(self.player_pos[1])][int(self.player_pos[0])] = \"X\"\n",
    "        print(\"\\n\".join(\"\".join(row) for row in reversed(grid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41165a03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward over 1000 episodes: -12.954\n",
      "Max reward over 1000 episodes: 7\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    \"length\": 10,\n",
    "    \"predator_step_size\": 4\n",
    "}\n",
    "num_episodes = 1000\n",
    "\n",
    "env = ChaseEnv(config)\n",
    "results = []\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    past_positions = []\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        action = random.randint(0, len(env.action_space) - 1)\n",
    "        result = env.step(action)\n",
    "        state, reward, done = result[\"state\"], result[\"r\"], result[\"done\"]\n",
    "        past_positions.append((env.player_pos, env.pred_pos))\n",
    "        total_reward += reward\n",
    "    results.append(total_reward)\n",
    "    # env.render(past_positions)\n",
    "\n",
    "print(f\"Average reward over {num_episodes} episodes: {sum(results) / len(results)}\")\n",
    "print(f\"Max reward over {num_episodes} episodes: {max(results)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "csc5661",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
